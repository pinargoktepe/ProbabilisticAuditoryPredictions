{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import mne\n",
    "from mne.forward import read_forward_solution\n",
    "from mne.minimum_norm import (make_inverse_operator, apply_inverse, write_inverse_operator)\n",
    "from mne.stats import summarize_clusters_stc\n",
    "\n",
    "import scipy\n",
    "import pickle\n",
    "\n",
    "from mayavi import mlab\n",
    "from IPython.display import Image\n",
    "\n",
    "from mne.decoding import get_coef\n",
    "\n",
    "mlab.init_notebook('png')\n",
    "mne.viz.set_3d_backend('mayavi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors = 'mag'\n",
    "tmin, tmax = -0.1, 0.6\n",
    "\n",
    "mainFolder = \"..\\Data\\\\\"\n",
    "print('main folder folder: ', mainFolder)\n",
    "\n",
    "meg_MainFolder = mainFolder + \"MEG_Data\\Data=\"\n",
    "print('meg main folder folder: ', meg_MainFolder)\n",
    "\n",
    "dataFolder = meg_MainFolder + str(tmin) + '_' + str(tmax) + '\\\\'\n",
    "print('Data folder: ', dataFolder)\n",
    "\n",
    "source_MainFolder = \"..\\SourceLocalization\\SourceEstimates\\Data=\"\n",
    "sourceFolder = source_MainFolder + str(tmin) + '_' + str(tmax) + '\\\\'\n",
    "print('Source folder: ', sourceFolder)\n",
    "\n",
    "subjects_dir = '..\\SourceLocalization\\subjects\\\\'\n",
    "print('Subjects directory: ', subjects_dir)\n",
    "\n",
    "forwardModelsFolder = '..\\SourceLocalization\\ForwardModels\\\\'\n",
    "print('Forward models folder: ', forwardModelsFolder)\n",
    "\n",
    "spatialFiltersFolder = '..\\SourceLocalization\\SpatialFilters\\\\Data='\n",
    "spatialFiltersFolder = spatialFiltersFolder + str(tmin) + '_' + str(tmax) + '\\\\'\n",
    "print('Spatial filters folder: ', spatialFiltersFolder)\n",
    "\n",
    "statResultsFolder  = '..\\SourceLocalization\\Results\\\\Data='\n",
    "statResultsFolder = statResultsFolder + str(tmin) + '_' + str(tmax) + '\\\\'\n",
    "print('Statisctics results folder: ', statResultsFolder)\n",
    "\n",
    "classifiers_MainFolder = \"..\\Classifiers\\Data=\"    \n",
    "clsfFolder = classifiers_MainFolder + str(tmin) + '_' + str(tmax) + '\\\\'\n",
    "print('Classifiers folder: ', clsfFolder)\n",
    "    \n",
    "\n",
    "# Pick the classifiers based on the their training window\n",
    "peak_indices = [20, 50]\n",
    "    \n",
    "print('Peak indices: ', peak_indices)\n",
    "    \n",
    "# Task name for the classifiers\n",
    "task_name = 'all_predLevel'\n",
    "print('Task name: ', task_name)\n",
    "\n",
    "# All subjects\n",
    "s_id_list_all = np.loadtxt(mainFolder+ids_filename, dtype=str)\n",
    "s_id_list_all = s_id_list_all.tolist()\n",
    "\n",
    "\n",
    "participant_names = np.loadtxt(mainFolder+participants_filename, dtype=str)\n",
    "participant_names = participant_names.tolist()\n",
    "\n",
    "\n",
    "print('Number of subjects: ', len(participant_names))\n",
    "\n",
    "if os.path.exists(subjects_dir + '\\\\' + participant_names[0] + '\\surf\\\\'):  \n",
    "    print('exists!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for loading/splitting data\n",
    "\n",
    "def getEpochData(s_id):\n",
    "    \n",
    "    if int(s_id) < 23:\n",
    "        fname = dataFolder+'S'+s_id+'\\\\'+s_id+'_2_tsss_mc_trans_'+sensors+'_nobase-epochs_afterICA'+filename_ext+'_manually_AR_resampled.fif'\n",
    "    else: \n",
    "        fname = dataFolder+'S'+s_id+'\\\\block_2_tsss_mc_trans_'+sensors+'_nobase-epochs_afterICA'+filename_ext+'_manually_AR_resampled.fif'\n",
    "\n",
    "\n",
    "    epochs = mne.read_epochs(fname, verbose='error')\n",
    "    print(fname + ' loaded!')\n",
    "    return epochs\n",
    "\n",
    "\n",
    "def splitData(epochs, events=None):\n",
    "    #print(epochs.event_id)\n",
    "    if events == None:\n",
    "        print('No events are given as parameter!')\n",
    "    \n",
    "    else:\n",
    "        print('Requested events: ', events)\n",
    "        return_list = epochs[events]\n",
    "     \n",
    "    print('Events in return list: ', return_list.event_id)\n",
    "    \n",
    "    return return_list\n",
    "\n",
    "\n",
    "def getClassifierWeghts(filename):\n",
    "\n",
    "    print('Classifier is loaded from ', filename)\n",
    "    loaded_model = []\n",
    "    with open(filename, \"rb\") as f:\n",
    "        while True:\n",
    "            try:\n",
    "                loaded_model.append(pickle.load(f))\n",
    "            except EOFError:\n",
    "                break\n",
    "        \n",
    "    return loaded_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inverse modeling with BeamFormer on evoked data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beamformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute spatial filters for beamformer\n",
    "\n",
    "def computeSpatialFilter(s_name, s_id, evoked, noise_cov, data_cov, condName):\n",
    "    print('Beamformer')\n",
    "    print('Computing spatial filter..')\n",
    "    \n",
    "    # Load forward model\n",
    "    fname_fwd = forwardModelsFolder + '\\\\fwd_sol_ico5_' + s_name + '.fif'\n",
    "    fwd = mne.read_forward_solution(fname_fwd, verbose=False)\n",
    "    \n",
    "    # Compute spatial filters with evoked data, forward model and covariance matrices\n",
    "    filters = mne.beamformer.make_lcmv(evoked.info, fwd, data_cov=data_cov, reg=0.05, noise_cov=noise_cov, \n",
    "                                       pick_ori='max-power', weight_norm='unit-noise-gain', rank='full', reduce_rank=True)\n",
    "   \n",
    "    # Save filters\n",
    "    filters.save(spatialFiltersFolder + s_id + '_filters-lcmv_' + s_name + '.h5', overwrite=True)\n",
    "    return filters\n",
    "    \n",
    "\n",
    "### Prepare data\n",
    "\n",
    "#### Morph subject's source estimate to template subject\n",
    "def morphToCommonSpace(stc, s_name, src_ave, smoothAmount=None):\n",
    "    print('Computing source morph..')\n",
    "    # Read the source space we are morphing to\n",
    "    fsave_vertices = [s['vertno'] for s in src_ave]\n",
    "\n",
    "    morph = mne.compute_source_morph(src=stc, subject_from=s_name, subject_to='fsaverage', \n",
    "                                     spacing=fsave_vertices, subjects_dir=subjects_dir, verbose=False,\n",
    "                                     smooth=smoothAmount)\n",
    "    tstep = stc.tstep\n",
    "    \n",
    "    print('Morphing data to fsaverage..')\n",
    "    stc_fsave = morph.apply(stc)\n",
    "    n_vertices_fsave = morph.morph_mat.shape[0]\n",
    "    \n",
    "    \n",
    "    return stc_fsave, n_vertices_fsave, tstep\n",
    "\n",
    "\n",
    "# generate the inverse solution for group average\n",
    "def prepareInverseSolution_group(data, tstep, tmin_tmp=0):\n",
    "    \n",
    "    src_ave = mne.read_source_spaces(subjects_dir+'fsaverage\\\\bem\\\\fsaverage-ico-5-src.fif')\n",
    "\n",
    "    fsave_vertices = [s['vertno'] for s in src_ave]        \n",
    "            \n",
    "    stc_return = mne.SourceEstimate(data, fsave_vertices, tmin_tmp, tstep, subject='fsaverage')\n",
    "    \n",
    "    \n",
    "    return stc_return\n",
    "    \n",
    "\n",
    "### Visualize results\n",
    "\n",
    "#The STC (Source Time Courses) are defined on a source space formed by 7498 candidate locations\n",
    "# and for a duration spanning 106 time instants.\n",
    "\n",
    "#Warning: Slide Type\n",
    "#!!PQt5 is necessary and also run jupyter nbextension enable mayavi --py on command line\n",
    "# before running the jupyter notebooks and also latest (6.1.1) version of module called 'traits'.\n",
    "\n",
    "def showResult(s_id, sourceFolder, stc, condName, minimum, maximum, tmin_tmp=0):\n",
    "    \n",
    "    initial_time = tmin_tmp\n",
    "    hemi_list = ['rh', 'lh']\n",
    "    for hemi in hemi_list:\n",
    "        print('Hemi: ', hemi)\n",
    "        \n",
    "        kwargs = dict(initial_time=initial_time, surface='inflated', hemi=hemi, subjects_dir=subjects_dir,\n",
    "                      verbose=True, size=(600, 600), spacing='all', background='w',\n",
    "                      cortex=(211/256,211/256,211/256))\n",
    "        \n",
    "        brain = stc.plot(**kwargs, colormap='Oranges')\n",
    "        brain.scale_data_colormap(fmin=minimum, fmid=(maximum+minimum)/2, fmax=maximum, transparent=True)\n",
    "        \n",
    "   \n",
    "        brain.show_view('lateral');\n",
    "\n",
    "        brain.save_image(sourceFolder + s_id + '_' + hemi + '_' + condName + '.png')\n",
    "\n",
    "        Image(filename = sourceFolder +  s_id + '_' + hemi + '_' + condName + '.png', width=600)\n",
    "\n",
    "\n",
    "def computeActivationMaps(model_list, epochs, tmin):\n",
    "\n",
    "    meg_data = epochs.get_data()\n",
    "    epochs.average().plot()\n",
    "    print(\"Meg data shape: \", meg_data.shape)\n",
    "    \n",
    "    # get classifier weights\n",
    "    if len(model_list) > 0:\n",
    "        model = model_list[0] # if model is loaded, it is stored in a list. Therefore we need to get model from index 0\n",
    "        \n",
    "        # Get classifier weights\n",
    "        coef = get_coef(model, 'coef_', inverse_transform=True)\n",
    "        \n",
    "        # Compute mean and std of weights\n",
    "        coef_mean = np.mean(coef)\n",
    "        coef_std = np.std(coef)\n",
    "        \n",
    "        # Standardize the weights\n",
    "        coef = (coef-coef_mean)/coef_std\n",
    "        print('shape of coef: ', coef.shape)\n",
    "    \n",
    "    \n",
    "    # Multiplying classifier weights with covariance of data to compute activation maps\n",
    "    activations_mat = np.zeros((meg_data.shape[0], meg_data.shape[1], meg_data.shape[2]))\n",
    "    # ntrials, nchannels, ntimes\n",
    "    \n",
    "    for t in range(meg_data.shape[2]):\n",
    "        epochs_tmp = epochs.copy()\n",
    "        epochs_tmp.crop(tmin=epochs.times[t], tmax=epochs.times[t])\n",
    "        cov_tmp = mne.compute_covariance(epochs_tmp, verbose=False)\n",
    "\n",
    "        activations = np.dot(coef, cov_tmp.data)\n",
    "        if t == 0:\n",
    "            print('Shape of activations: ', activations.shape)\n",
    "        \n",
    "        for i in range( meg_data.shape[0]):\n",
    "            activations_mat[i, :,t] = activations.reshape(meg_data.shape[1])\n",
    "            \n",
    "        del cov_tmp \n",
    "    \n",
    "    # Simulate epoch object with activation maps\n",
    "    epoched_sim = mne.EpochsArray(activations_mat, epochs.info, tmin=tmin)\n",
    "\n",
    "    return epoched_sim\n",
    "\n",
    "### Apply Beamformer\n",
    "def applyBeamformer(conditions, s_id_list_all, n_subjects, participant_names,\n",
    "                    tminData, tmaxData, tminNoise, tmaxNoise, tminEpoch, smoothAmount, task_name):\n",
    "    \n",
    "    stc_fsave_all_real, n_times = None,  None\n",
    "\n",
    "\n",
    "    for s in range(n_subjects): \n",
    "        s_id = s_id_list_all[s]\n",
    "        s_name = participant_names[s]\n",
    "        print(' ------------- ' + s_name + ' ------------- ')\n",
    "        epochs = getEpochData(s_id)\n",
    "        print(epochs.event_id)\n",
    "        print('epochs shape: ', epochs.get_data().shape)\n",
    "        \n",
    "        # check if all conditions exist in the epoch (e.g. omissions_living_nores might not exist!)\n",
    "        conditions = [c for c in conditions if c in epochs.event_id]\n",
    "        print('Final conditions: ', conditions)\n",
    "        splits = epochs[conditions]\n",
    "        print('Events in splits: ', splits.event_id)\n",
    "        \n",
    "                \n",
    "        # Load classifier weights to compute activation maps\n",
    "        clsf_model_filename = clsfFolder+'S'+s_id+'\\\\'+s_id+'_clsf_'+task_name+'_'+str(peak_indices[0])+'_'+str(peak_indices[1])+'_sm.sav'\n",
    "        clsf_model = getClassifierWeghts(clsf_model_filename)\n",
    "\n",
    "        # compute activation maps and simulate epoch object for source localization\n",
    "        print('Compute activations')\n",
    "        epoch_sim = computeActivationMaps(clsf_model, splits, tmin=tminEpoch)\n",
    "\n",
    "        print('Compute noise covariance')\n",
    "        noise_cov = mne.compute_covariance(epoch_sim, tmin=tminNoise, tmax=tmaxNoise, \n",
    "                                           method=['shrunk', 'empirical'], verbose=False) \n",
    "        print('Compute data covariance')\n",
    "        data_cov = mne.compute_covariance(epoch_sim, tmin=tminData, tmax=tmaxData, \n",
    "                                          method=['shrunk', 'empirical'], verbose=False)\n",
    "\n",
    "        # compute average of epochs\n",
    "        evoked = epoch_sim.average().crop(tmin=tminData, tmax=tmaxData)\n",
    "        print('Shape of evoked data: ', evoked._data.shape)\n",
    "\n",
    "        # computer spatial filters by LCMV\n",
    "        print('Compute filter: ')\n",
    "        filters = computeSpatialFilter(s_name, s_id, evoked, noise_cov, data_cov, conditions)\n",
    "        \n",
    "        print('Apply beamformer: ')\n",
    "        stc = mne.beamformer.apply_lcmv(evoked=evoked, filters=filters, max_ori_out='signed')\n",
    "        n_vertices_sample, n_times = stc.data.shape\n",
    "       \n",
    "\n",
    "        if s_id != '16' and s_id != '31' and s_id != '40': # for participants with MR data \n",
    "            stc_fsave, n_vertices_fsave, tstep = morphToCommonSpace(stc, s_name, src_ave,\n",
    "                                                                    smoothAmount=smoothAmount)\n",
    "        else: # for participants without MR data \n",
    "            stc_fsave = stc\n",
    "            \n",
    "        # initialize the stc data array when computing the first participant\n",
    "        if s==0:\n",
    "            print('n_times: ', n_times)\n",
    "            stc_fsave_all_real = np.zeros((n_vertices_fsave, n_times, n_subjects),)\n",
    "\n",
    "        \n",
    "        stc_fsave_all_real[:,:,s] = np.abs(stc_fsave.data.reshape(n_vertices_fsave, n_times))\n",
    "\n",
    "\n",
    "\n",
    "    return  stc_fsave_all_real, n_times, tstep\n",
    "                    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read fsaverage\n",
    "src_ave = mne.read_source_spaces(subjects_dir + 'fsaverage\\\\bem\\\\fsaverage-ico-5-src.fif')\n",
    "\n",
    "\n",
    "\n",
    "#### Choose the conditions for contrasting\n",
    "\n",
    "conditions = ['living_real_8', 'living_real_9', 'living_real_10', 'object_real_8', 'object_real_9', 'object_real_10']\n",
    "condition_names =  [\"real\"]\n",
    "\n",
    "n_subjects = len(s_id_list_all)\n",
    "print('number of subjects: ', n_subjects)\n",
    "\n",
    "print('conditions: \\n', conditions)\n",
    "\n",
    "\n",
    "### Define time limits for inverse solutins\n",
    "\n",
    "tminData, tmaxData = 0.1, 0.4  #0.1, 0.4 #-0.04, 0\n",
    "print('tmin for data: ', tminData)\n",
    "print('tmax for data: ', tmaxData)\n",
    "tminNoiseCov, tmaxNoiseCov = -0.1, -0.05\n",
    "print('tmin for noise covariance: ', tminNoiseCov)\n",
    "print('tmax for noise covariance: ', tmaxNoiseCov)\n",
    "tminEpoch = -0.1\n",
    "print('tmin for generated epochs: ', tminEpoch)\n",
    "smoothAmount = 70\n",
    "print('Smoothing amount: ', smoothAmount)\n",
    "inv_sol_method = 'beamformer'\n",
    "print('Inverse solution method: ', inv_sol_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use beamformer for computing source estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('======= Applying Beamformer ========')\n",
    "\n",
    "stc_real_filename = 'stc_fsave_allTogether_real_onActivationMaps_'+inv_sol_method+'_'+str(tminData)+'_'+str(tmaxData)+'_sm='+str(smoothAmount)+'.npy'\n",
    "\n",
    "if os.path.exists(stc_real_filename != True) :  # Compute the source estimate\n",
    "\n",
    "    stc_fsave_all_real, n_times, tstep = applyBeamformer(conditions, s_id_list_all, \n",
    "                                                         n_subjects, participant_names, tminData, tmaxData, \n",
    "                                                         tminNoiseCov, tmaxNoiseCov, tminEpoch, smoothAmount, task_name)\n",
    "\n",
    "    print('stc is saved in ', stc_real_filename)\n",
    "    np.save(statResultsFolder + stc_real_filename, stc_fsave_all_real)\n",
    "\n",
    "    # Check if there is 0\n",
    "    if len(np.where(stc_fsave_all_real == 0)[0]) == 0:\n",
    "        print('No Zeros!')\n",
    "    else:\n",
    "        print('There are zeros at ', np.where(stc_fsave_all_real == 0)[0])\n",
    "\n",
    "else:\n",
    "    tstep=0.01\n",
    "    # time period of interest\n",
    "    tminData_cls_tmp, tmaxData_cls_tmp = 0.1, 0.4\n",
    "    stc_real_filename = 'stc_fsave_allTogether_real_onActivationMaps_'+inv_sol_method+'_'+str(tminData)+'_'+str(tmaxData)+'_sm='+str(smoothAmount)+'.npy'\n",
    "    print('Source estimate exists!\\n Loading from file'+statResultsFolder+stc_real_filename+'...')\n",
    "    stc_fsave_all_real_clfRange = np.load(stc_real_filename)\n",
    "    print('shape of real sounds source estimates: ', stc_fsave_all_real_clfRange.shape)\n",
    "    print('There are zeros at ', np.where(stc_fsave_all_real_clfRange == 0))\n",
    "    \n",
    "    # Baseline\n",
    "    tminData_baseline_tmp, tmaxData_baseline_tmp = -0.04, 0\n",
    "    stc_baseline_filename = 'stc_fsave_allTogether_real_onActivationMaps_'+inv_sol_method+'_'+str(tminData_baseline_tmp)+'_'+str(tmaxData_baseline_tmp)+'_sm='+str(smoothAmount)+'.npy'\n",
    "    print('Source estimate exists!\\n Loading from file'+stc_baseline_filename+'...')\n",
    "    print(statResultsFolder+stc_baseline_filename)\n",
    "    stc_fsave_all_real_baseline = np.load(statResultsFolder+stc_baseline_filename)\n",
    "    print('shape of real sounds source estimates: ', stc_fsave_all_real_baseline.shape)\n",
    "    print('There are zeros at ',np.where(stc_fsave_all_real_baseline == 0))\n",
    "\n",
    "\n",
    "### Check data before doing anything!!\n",
    "\n",
    "print('Baseline: \\n', stc_fsave_all_real_baseline)\n",
    "\n",
    "print('Clf range: \\n', stc_fsave_all_real_clfRange)\n",
    "\n",
    "### Take mean \n",
    "# take mean over participants (dimension 2)\n",
    "stc_fsave_all_real_clfRange_avg = np.mean(stc_fsave_all_real_clfRange, axis=2)\n",
    "print('Shape stc_fsave_all_real_clfRange_avg after avg over subjects: ', stc_fsave_all_real_clfRange_avg.shape)\n",
    "# take mean over time points (dimension 1)\n",
    "stc_fsave_all_real_clfRange_avg= np.mean(stc_fsave_all_real_clfRange_avg, axis=1)\n",
    "print('Shape stc_fsave_all_real_clfRange_avg after avg across time: ', stc_fsave_all_real_clfRange_avg.shape)\n",
    "stc_fsave_all_real_clfRange_avg.shape\n",
    "\n",
    "print('stc_fsave_all_real_clfRange_avg: ', stc_fsave_all_real_clfRange_avg)\n",
    "\n",
    "# take mean over participants (dimension 2)\n",
    "stc_fsave_all_real_baseline_avg = np.mean(stc_fsave_all_real_baseline, axis=2)\n",
    "print('Shape stc_fsave_all_real_baseline_avg after avg over subjects: ', stc_fsave_all_real_baseline_avg.shape)\n",
    "# take mean over time points (dimension 1)\n",
    "stc_fsave_all_real_baseline_avg = np.mean(stc_fsave_all_real_baseline_avg, axis=1)\n",
    "print('Shape stc_fsave_all_real_baseline_avg after avg across time: ', stc_fsave_all_real_baseline_avg.shape)\n",
    "stc_fsave_all_real_baseline_avg.shape\n",
    "\n",
    "print('stc_fsave_all_real_baseline_avg: ', stc_fsave_all_real_baseline_avg)\n",
    "\n",
    "### Generate stc group\n",
    "stc_cls = prepareInverseSolution_group(stc_fsave_all_real_clfRange_avg, tstep=tstep, tmin_tmp=tminData_cls_tmp)\n",
    "stc_baseline = prepareInverseSolution_group(stc_fsave_all_real_baseline_avg, tstep=tstep,\n",
    "                                            tmin_tmp=tminData_baseline_tmp)\n",
    "\n",
    "\n",
    "### Plot\n",
    "\n",
    "'real_baseline_allTogether_'+inv_sol_method+'_sm='+str(smoothAmount)\n",
    "\n",
    "showResult('fsaverage', sourceFolder, stc_cls,\n",
    "           'real_clf_allTogether_'+inv_sol_method+'_sm='+str(smoothAmount), \n",
    "           minimum=1300000, maximum=3000000)\n",
    "\n",
    "showResult('fsaverage', sourceFolder, stc_baseline, \n",
    "           'real_baseline_allTogether_'+inv_sol_method+'_sm='+str(smoothAmount), \n",
    "           minimum=99000, maximum=440000)\n",
    "\n",
    "# Compute relative change from baseline\n",
    "stc_fsave_all_real_diff_avg = 100*(stc_fsave_all_real_clfRange_avg-stc_fsave_all_real_baseline_avg)/stc_fsave_all_real_baseline_avg\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "#print(np.min(stc_fsave_all_real_diff_avg))\n",
    "\n",
    "# Threshold for plotting\n",
    "thresh= 700 #550\n",
    "print('Shape of data larger than threshold: ', np.where(stc_fsave_all_real_diff_avg >thresh)[0].shape)\n",
    "# zero out the values below threshold\n",
    "for i in range(stc_fsave_all_real_diff_avg.shape[0]):\n",
    "    if stc_fsave_all_real_diff_avg[i] < thresh:\n",
    "        stc_fsave_all_real_diff_avg[i] = 0\n",
    "\n",
    "stc_diff = prepareInverseSolution_group(stc_fsave_all_real_diff_avg, tstep=tstep, tmin_tmp=tminData_cls_tmp)\n",
    "\n",
    "#print(np.max(stc_diff.data))\n",
    "# Visualize the thresholded data\n",
    "showResult('fsaverage', sourceFolder, stc_diff, \n",
    "           '2real_change_allTogether_'+inv_sol_method+'_sm='+str(smoothAmount) +'_pthresh=' + str(thresh), \n",
    "           minimum=thresh, maximum=1100)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
