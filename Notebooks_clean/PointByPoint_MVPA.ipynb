{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old name of the notebook: OmissionsMEGAnalysis_PredictabilityLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "from mne import find_events\n",
    "from mne.decoding import Vectorizer, SlidingEstimator, cross_val_multiscore\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "random.seed(42)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split, StratifiedKFold, RepeatedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, classification_report, accuracy_score, precision_recall_fscore_support, balanced_accuracy_score\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#%matplotlib tk\n",
    "import pickle\n",
    "from scipy.stats import sem\n",
    "from scipy.stats import wilcoxon\n",
    "\n",
    "import import_ipynb\n",
    "from CommonFunctions import loadData, extractDataAndLabels, concatNonEmpty, trainAndTest_MVPA, plot_MVPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors = 'mag'\n",
    "\n",
    "#File paths\n",
    "meg_MainFolder = \"..\\Data\\MEG_Data\\Data=\"\n",
    "figures_MainFolder = \"..\\Figures\\MVPA\\Data=\"\n",
    "results_MainFolder = \"..\\Results\\Data=\"\n",
    "classifiers_MainFolder = \"..\\Classifiers\\Data=\"\n",
    "\n",
    "tmin, tmax = -0.1, 0.6\n",
    "\n",
    "# Decide the time limit based on the time range of the data\n",
    "tlim = 0\n",
    "if tmin == -0.8:\n",
    "    tlim = 140\n",
    "elif tmin == -0.1:\n",
    "    tlim = 70\n",
    "\n",
    "print('tmin = ', tmin)\n",
    "print('tmax = ', tmax)\n",
    "\n",
    "dataFolder = meg_MainFolder + str(tmin) + '_' + str(tmax) + '\\\\'\n",
    "print('Data folder: ', dataFolder)\n",
    "\n",
    "figuresFolder = figures_MainFolder + str(tmin) + '_' + str(tmax) + '\\\\'\n",
    "print('Figures folder: ', figuresFolder)\n",
    "\n",
    "results_folder = results_MainFolder + str(tmin) + '_' + str(tmax) + '\\\\'\n",
    "print('Results folder: ', results_folder)\n",
    "\n",
    "clsfFolder = classifiers_MainFolder + str(tmin) + '_' + str(tmax) + '\\\\'\n",
    "print('Classifiers folder: ', clsfFolder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_ext = ''\n",
    "if tmin == -0.8 and tmax == 0.6:\n",
    "    filename_ext = '-elongated'\n",
    "elif tmin == -0.8 and tmax == 1:\n",
    "    filename_ext = '-elongated_2'\n",
    "    \n",
    "print('filename ext: ', filename_ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_ids = ['13', '16', '17', '18', '21', '23', '26', '28', '29', '30', '31', '32',\n",
    "         '33', '34', '35', '36', '38', '39', '40', '41', '42']\n",
    "\n",
    "print('Number of subjects: ', len(s_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_results = []\n",
    "\n",
    "for s_id in s_ids:\n",
    "\n",
    "    print('------------ ' + s_id + '------------ ')\n",
    "    if int(s_id) < 23:\n",
    "        fname = dataFolder+'S'+s_id+'\\\\'+s_id+'_2_tsss_mc_trans_'+sensors+'_nobase-epochs_afterICA'+filename_ext+'_manually_AR.fif'\n",
    "    else: \n",
    "        fname = dataFolder+'S'+s_id+'\\\\block_2_tsss_mc_trans_'+sensors+'_nobase-epochs_afterICA'+filename_ext+'_manually_AR.fif'\n",
    "\n",
    "    # Check if data is resampled already\n",
    "    if os.path.isfile(fname[:-4]+'_resampled.fif'): \n",
    "        print('Already resampled data!')\n",
    "        epochs = loadData(s_id, sensors, fname[:-4]+'_resampled.fif', resampled=True)\n",
    "    \n",
    "    # If not, resample\n",
    "    else:\n",
    "        print('Data will be resampled!')\n",
    "        epochs = loadData(s_id, sensors, fname, resampled=False)\n",
    "    \n",
    "    print('Number of epochs: ', len(epochs))\n",
    "    #Split data\n",
    "    \n",
    "    #Real data and labels\n",
    "    data_real_living, labels_real_living = extractDataAndLabels(epochs, ['living_real_8', 'living_real_9', 'living_real_10'])\n",
    "    data_real_object, labels_real_object = extractDataAndLabels(epochs, ['object_real_8', 'object_real_9', 'object_real_10'])\n",
    "\n",
    "    #living omission data and labels\n",
    "    data_omission_living, labels_omission_living = extractDataAndLabels(epochs, ['living_omission_8_corr', 'living_omission_8_incorr', 'living_omission_9_corr', 'living_omission_9_incorr', 'living_omission_10_corr', 'living_omission_10_incorr'])\n",
    "\n",
    "    #object omission data and labels\n",
    "    data_omission_object, labels_omission_object = extractDataAndLabels(epochs, ['object_omission_8_corr', 'object_omission_8_incorr', 'object_omission_9_corr', 'object_omission_9_incorr', 'object_omission_10_corr', 'object_omission_10_incorr'])\n",
    "\n",
    "    \n",
    "    #Real sounds 80% predictibility\n",
    "    data_real_8 = concatNonEmpty([data_real_living[0], data_real_object[0]])\n",
    "    labels_real_8 = concatNonEmpty([labels_real_living[0], labels_real_object[0]])\n",
    "\n",
    "    #Real sounds 90% predictibility\n",
    "    data_real_9 = concatNonEmpty([data_real_living[1], data_real_object[1]])\n",
    "    labels_real_9 = concatNonEmpty([labels_real_living[1], labels_real_object[1]])\n",
    "\n",
    "    #Real sounds 100% predictibility\n",
    "    data_real_10 = concatNonEmpty([data_real_living[2], data_real_object[2]])\n",
    "    labels_real_10 = concatNonEmpty([labels_real_living[2], labels_real_object[2]])\n",
    "\n",
    "    #All levels together\n",
    "\n",
    "    data_real_all = concatNonEmpty([data_real_8, data_real_9, data_real_10])\n",
    "    labels_real_all = concatNonEmpty([labels_real_8, labels_real_9, labels_real_10])\n",
    "\n",
    "\n",
    "    #omission sounds\n",
    "    #80% predictibility\n",
    "    data_omission_8= concatNonEmpty([data_omission_living[0], data_omission_living[1], data_omission_object[0], data_omission_object[1]])\n",
    "    labels_omission_8 = concatNonEmpty([labels_omission_living[0], labels_omission_living[1], labels_omission_object[0], labels_omission_object[1]])\n",
    "\n",
    "    #90% predictibility\n",
    "    data_omission_9 = concatNonEmpty([data_omission_living[2], data_omission_living[3], data_omission_object[2], data_omission_object[3]])\n",
    "    labels_omission_9 = concatNonEmpty([labels_omission_living[2], labels_omission_living[3], labels_omission_object[2], labels_omission_object[3]])\n",
    "\n",
    "    #100% predictibility\n",
    "    data_omission_10 = concatNonEmpty([data_omission_living[4], data_omission_living[5], data_omission_object[4], data_omission_object[5]])\n",
    "    labels_omission_10 = concatNonEmpty([labels_omission_living[4], labels_omission_living[5], labels_omission_object[4], labels_omission_object[5]])\n",
    "\n",
    "\n",
    "    ## MVPA \n",
    "    \n",
    "    # NOTE: Below part in comment is for classification on each predictability level. If you don't want to test this, \n",
    "    # skip below part and go to line 116 where classification on all levels together starts\n",
    "    '''\n",
    "    results = []\n",
    "\n",
    "    outputfilename_8 = results_folder+s_id + \"_\" + sensors + \"_results_8_predLevel\"\n",
    "    bestParametersFile_8 = results_folder + s_id + \"_\" + sensors + \"_bestParametes_8_predLevel.txt\"\n",
    "    clsfFile = clsfFolder + \"_8_predLevel.pkl\"\n",
    "    results_8 = trainAndTest_MVPA(data_real_8, labels_real_8, [data_omission_8], [labels_omission_8], outputfilename_8, bestParametersFile_8, tlim, clsfFile, nFolds=5)\n",
    "    results.append(results_8)\n",
    "\n",
    "    plotname_8 = filep_figures + 'MVPA_S' + s_id+'_' + sensors + '_8_predLevel.png'\n",
    "    plot_MVPA(results_8, epochs.times[:tlim], tlim, plotname_8)\n",
    "\n",
    "    outputfilename_9 = results_folder+s_id + \"_\" + sensors + \"_results_9_predLevel\"\n",
    "    bestParametersFile_9 = results_folder + s_id + \"_\" + sensors + \"_bestParametes_9_predLevel.txt\"\n",
    "    clsfFile = clsfFolder + \"_9_predLevel.pkl\"\n",
    "    results_9 = trainAndTest_MVPA(data_real_9, labels_real_9, [data_omission_9], [labels_omission_9], outputfilename_9, bestParametersFile_9, tlim, clsfFile, nFolds=5)\n",
    "    results.append(results_9) \n",
    "\n",
    "    plotname_9 = filep_figures + 'MVPA_S' + s_id+'_' + sensors + '_9_predLevel.png'\n",
    "    plot_MVPA(results_9,  epochs.times[:tlim], tlim, plotname_9)\n",
    "\n",
    "\n",
    "    outputfilename_10 = results_folder+s_id + \"_\" + sensors + \"_results_10_predLevel\"\n",
    "    bestParametersFile_10 = results_folder + s_id + \"_\" + sensors + \"_bestParametes_10_predLevel.txt\"\n",
    "    clsfFile = clsfFolder + \"_10_predLevel.pkl\"\n",
    "\n",
    "    results_10 = trainAndTest_MVPA(data_real_10, labels_real_10, [data_omission_10], [labels_omission_10], outputfilename_10, bestParametersFile_10, tlim, clsfFile, nFolds=5)\n",
    "    results.append(results_10) \n",
    "\n",
    "    plotname_10 = filep_figures + 'MVPA_S' + s_id+'_' + sensors + '_10_predLevel.png'\n",
    "    plot_MVPA(results_10,  epochs.times[:tlim], tlim, plotname_10)\n",
    "\n",
    "\n",
    "\n",
    "    outputfilename_all = results_folder+s_id + \"_\" + sensors + \"_results_all_predLevel_linearKernel\"\n",
    "    bestParametersFile_all = results_folder + s_id + \"_\" + sensors + \"_bestParameters_all_linearKernel.txt\"\n",
    "    clsfFile = clsfFolder + \"all_predLevel.pkl\"\n",
    "    print(clsfFile)\n",
    "\n",
    "    '''\n",
    "    outputfilename_all = results_folder+ '\\\\S' + s_id + '\\\\' + s_id + \"_\" + sensors + \"_results_all_predLevel\"\n",
    "    print(outputfilename_all)\n",
    "    bestParametersFile_all = results_folder + '\\\\S' + s_id + '\\\\' + s_id + \"_\" + sensors + \"_bestParameters_all.txt\"\n",
    "    clsfFile = clsfFolder + s_id + \"\\\\all_predLevel.pkl\"\n",
    "    results_all = trainAndTest_MVPA(data_real_all, labels_real_all,\n",
    "                                    [data_omission_8, data_omission_9, data_omission_10], \n",
    "                                    [labels_omission_8, labels_omission_9, labels_omission_10], \n",
    "                                    outputfilename_all, bestParametersFile_all, tlim, clsfFile, nFolds=5,\n",
    "                                    bestParamsFound=False)\n",
    "    \n",
    "    \n",
    "    #results.append(results_all) \n",
    "\n",
    "    plotname_all = figuresFolder + 'S' + s_id + '\\\\MVPA_S' + s_id+'_' + sensors + '_all_predLevel.png'\n",
    "    print(plotname_all)\n",
    "    plot_MVPA(results_all, epochs.times[:tlim], tlim, plotname_all)\n",
    "    \n",
    "    group_results.append(np.asarray(results_all))\n",
    "    \n",
    "    del data_real_living, data_real_object, data_omission_living, data_omission_object\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load results if you have them already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load one subjects epoch data to use times\n",
    "s_id = '17'\n",
    "fname = dataFolder+'S'+s_id+'\\\\'+s_id+'_2_tsss_mc_trans_'+sensors+'_nobase-epochs_afterICA'+filename_ext+'_manually_AR_resampled.fif'\n",
    "epochs = mne.read_epochs(fname)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_results = []\n",
    "\n",
    "for s_id in s_ids:\n",
    "    outputfilename = results_folder+ '\\\\S' + s_id + '\\\\' + s_id + \"_\" + sensors + \"_results_all_predLevel.npy\"\n",
    "    res_tmp = np.load(outputfilename, allow_pickle=True)\n",
    "    print('Shape of loaded results: ', res_tmp.shape)\n",
    "    group_results.append(res_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_results = np.asarray(group_results)\n",
    "group_results[:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_results[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_results[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check results if there are 0s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#print('is there any 0: ', np.where(np.sum(group_results, axis = 1) == 0))\n",
    "group_real = np.zeros((len(s_ids),tlim))\n",
    "for i in range(group_results[:,0].shape[0]): # use only the index 0 as it keeps the real sonds results and the rest is omissions and CV\n",
    "    group_real[i,:] = group_results[i][0]\n",
    "print(group_real.shape)  \n",
    "\n",
    "group_avg_real = np.nanmean(group_real, axis=0)\n",
    "group_std_real = sem(group_real, axis = 0, nan_policy='omit')/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('is there any 0: ', np.where(group_results[:,0] == 0))\n",
    "group_real = np.zeros((len(s_ids),tlim))\n",
    "for i in range(group_results[:,0].shape[0]): # use only the index 0 as it keeps the real sonds results and the rest is omissions and CV\n",
    "\n",
    "    group_real[i,:] = group_results[i][0]\n",
    "print(group_real.shape)  \n",
    "\n",
    "group_avg_real = np.nanmean(group_real, axis=0)\n",
    "group_std_real = sem(group_real, axis = 0, nan_policy='omit')/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_real.shape\n",
    "np.save(results_folder+'real_sounds_testResults_pointBypointAnalysis', group_real)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply stats to compare against chance level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_threshold = 0.004\n",
    "print('p value threshold: ', p_threshold)\n",
    "nt = len(epochs.times)\n",
    "preal = np.ones(nt)\n",
    "chance = 0.5\n",
    "for t in np.arange(nt):\n",
    "    x, preal[t] = wilcoxon(group_real[:,t]-chance, alternative='greater')\n",
    "    \n",
    "print('\\nP values: ', preal)\n",
    "\n",
    "max_score = np.max(group_avg_real)\n",
    "time_max_score = epochs.times[np.where(group_avg_real == max_score)[0].tolist()]\n",
    "print('\\nMaximum score of ' + str(max_score)+' achieved at ' + str(time_max_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne.stats import bonferroni_correction\n",
    "r,p_corrected = mne.stats.bonferroni_correction(preal)\n",
    "print('P values before the correction: ', preal)\n",
    "print('P values after the correction: ', p_corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_sig = epochs.times[np.where(preal<= p_threshold)[0].tolist()]\n",
    "print('\\nSignificant time points before correction: ', times_sig)\n",
    "\n",
    "times_sig = epochs.times[np.where(p_corrected <= p_threshold)[0].tolist()]\n",
    "print('\\nSignificant time points after correction: ', times_sig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "color = np.array((27,120,55))/256\n",
    "color_chanceLevel = np.array((169,169,169))/256\n",
    "\n",
    "al = 0.2\n",
    "font_size = 14\n",
    "\n",
    "fig = plt.figure(num=None, figsize=(6, 4), dpi=150)\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "\n",
    "plt.plot(epochs.times, group_avg_real, color = color, linewidth = 4, label='Real Sounds')\n",
    "plt.fill_between(epochs.times, group_avg_real, group_avg_real + group_std_real, color=color,\n",
    "                 interpolate=True, alpha = al)\n",
    "plt.fill_between(epochs.times, group_avg_real, group_avg_real - group_std_real, color=color,\n",
    "                 interpolate=True, alpha = al)\n",
    "\n",
    "plt.xlabel('Time(s)', fontsize=font_size)\n",
    "plt.ylabel('AUC', fontsize=font_size)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "\n",
    "timeInterval = 0.01\n",
    "print('TIme between two time points: ', timeInterval)\n",
    "\n",
    "if len(times_sig)>0:\n",
    "    times_sig_periods = []\n",
    "    print('Significant time points exist!')\n",
    "    \n",
    "    # Computet the time periods that are significcantt\n",
    "    start, end = None, None\n",
    "    for k in range(len(times_sig)):\n",
    "        if k == 0:\n",
    "            start = times_sig[k]\n",
    "            if len(times_sig) == 1: # if we have only 1 time point that is significant\n",
    "                end = times_sig[k]\n",
    "                times_sig_periods.append([start,end])\n",
    "        else:\n",
    "            if round(times_sig[k],2) != round(times_sig[k-1] + timeInterval, 2): # if the data points are not continuous\n",
    "                end =  times_sig[k-1]\n",
    "                times_sig_periods.append([start,end])\n",
    "                start = times_sig[k]\n",
    "            else:\n",
    "                if k == len(times_sig)-1:\n",
    "                    print('the end of computation!')\n",
    "                    end = times_sig[k]\n",
    "                    times_sig_periods.append([start,end])\n",
    "\n",
    "                \n",
    "    print('Significant time intervals: ', times_sig_periods) \n",
    "    \n",
    "    for p in range(len(times_sig_periods)):\n",
    "        if times_sig_periods[p][0] == times_sig_periods[p][1]:\n",
    "            print('Here')\n",
    "            ax.plot(times_sig_periods[p][0], 0.49)\n",
    "        else:\n",
    "            ax.hlines(xmin=times_sig_periods[p][0], xmax=times_sig_periods[p][1], y=0.49, color=color, linestyle='-')\n",
    "    \n",
    "    ax.hlines(xmin=epochs.times[0], xmax=epochs.times[-1], y=0.5, color=color_chanceLevel, linestyle='--', label='Chance')\n",
    "    plt.ylim(0.4, 0.78)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "    plt.savefig(figures_MainFolder+'Group_Level_real_sounds_timePointByTimePoint_bonfr_crr_p='+str(p_threshold)+'.png',  bbox_inches='tight')\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('is there any 0: ', np.where(group_results[:,-1] == 0))\n",
    "group_real = np.zeros((len(s_ids),tlim))\n",
    "for i in range(group_results[:,-1].shape[0]): # use only the index 0 as it keeps the real sonds results and the rest is omissions and CV\n",
    "\n",
    "    group_real[i,:] = np.mean(group_results[i][-1], axis=0)\n",
    "print(group_real.shape)  \n",
    "\n",
    "group_avg_real = np.nanmean(group_real, axis=0)\n",
    "group_std_real = sem(group_real, axis = 0, nan_policy='omit')/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_threshold = 0.001\n",
    "print('p value threshold: ', p_threshold)\n",
    "nt = len(epochs.times)\n",
    "preal = np.ones(nt)\n",
    "chance = 0.5\n",
    "for t in np.arange(nt):\n",
    "    x, preal[t] = wilcoxon(group_real[:,t]-chance, alternative='greater')\n",
    "    \n",
    "print('\\nP values: ', preal)\n",
    "\n",
    "max_score = np.max(group_avg_real)\n",
    "time_max_score = epochs.times[np.where(group_avg_real == max_score)[0].tolist()]\n",
    "print('\\nMaximum score of ' + str(max_score)+' achieved at ' + str(time_max_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne.stats import bonferroni_correction\n",
    "r,p_corrected = mne.stats.bonferroni_correction(preal)\n",
    "print('P values before the correction: ', preal)\n",
    "print('P values after the correction: ', p_corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_sig_wo_corr = epochs.times[np.where(preal<= p_threshold)[0].tolist()]\n",
    "print('\\nSignificant time points before correction: ', times_sig_wo_corr)\n",
    "\n",
    "times_sig = epochs.times[np.where(p_corrected <= p_threshold)[0].tolist()]\n",
    "print('\\nSignificant time points after correction: ', times_sig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "color = np.array((27,120,55))/256\n",
    "color_chanceLevel = np.array((169,169,169))/256\n",
    "\n",
    "al = 0.2\n",
    "font_size = 14\n",
    "\n",
    "fig = plt.figure(num=None, figsize=(6, 4), dpi=150)\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "\n",
    "plt.plot(epochs.times, group_avg_real, color = color, linewidth = 4, label='Real Sounds')\n",
    "plt.fill_between(epochs.times, group_avg_real, group_avg_real + group_std_real, color=color,\n",
    "                 interpolate=True, alpha = al)\n",
    "plt.fill_between(epochs.times, group_avg_real, group_avg_real - group_std_real, color=color,\n",
    "                 interpolate=True, alpha = al)\n",
    "\n",
    "plt.xlabel('Time(s)', fontsize=font_size)\n",
    "plt.ylabel('AUC', fontsize=font_size)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "\n",
    "timeInterval = 0.01\n",
    "print('TIme between two time points: ', timeInterval)\n",
    "\n",
    "if len(times_sig)>0:\n",
    "    times_sig_periods = []\n",
    "    print('Significant time points exist!')\n",
    "    \n",
    "    # Computet the time periods that are significcantt\n",
    "    start, end = None, None\n",
    "    for k in range(len(times_sig)):\n",
    "        if k == 0:\n",
    "            start = times_sig[k]\n",
    "            if len(times_sig) == 1: # if we have only 1 time point that is significant\n",
    "                end = times_sig[k]\n",
    "                times_sig_periods.append([start,end])\n",
    "        else:\n",
    "            if round(times_sig[k],2) != round(times_sig[k-1] + timeInterval, 2): # if the data points are not continuous\n",
    "                end =  times_sig[k-1]\n",
    "                times_sig_periods.append([start,end])\n",
    "                start = times_sig[k]\n",
    "            else:\n",
    "                if k == len(times_sig)-1:\n",
    "                    print('the end of computation!')\n",
    "                    end = times_sig[k]\n",
    "                    times_sig_periods.append([start,end])\n",
    "\n",
    "                \n",
    "    print('Significant time intervals: ', times_sig_periods) \n",
    "    \n",
    "    for p in range(len(times_sig_periods)):\n",
    "        if times_sig_periods[p][0] == times_sig_periods[p][1]:\n",
    "            print('Here')\n",
    "            ax.plot(times_sig_periods[p][0], 0.49)\n",
    "        else:\n",
    "            ax.hlines(xmin=times_sig_periods[p][0], xmax=times_sig_periods[p][1], y=0.49, color=color, linestyle='-')\n",
    "    \n",
    "    ax.hlines(xmin=epochs.times[0], xmax=epochs.times[-1], y=0.5, color=color_chanceLevel, linestyle='--', label='Chance')\n",
    "    \n",
    "    plt.ylim(0.4, 0.78)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "    plt.savefig(figures_MainFolder+'CV_Group_Level_real_sounds_timePointByTimePoint_bonfr_crr_p='+str(p_threshold)+'.png',  bbox_inches='tight')\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
