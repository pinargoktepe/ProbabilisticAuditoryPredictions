{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import mne\n",
    "from mne.forward import read_forward_solution\n",
    "from mne.minimum_norm import (make_inverse_operator, apply_inverse, write_inverse_operator)\n",
    "from mne.stats import summarize_clusters_stc\n",
    "\n",
    "import scipy\n",
    "from scipy import stats\n",
    "import pickle\n",
    "\n",
    "from mayavi import mlab\n",
    "from IPython.display import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors = 'mag'\n",
    "meg_MainFolder = \"..\\Data\\MEG_Data\\Data=\"\n",
    "tmin, tmax = -0.1, 0.6\n",
    "\n",
    "dataFolder = meg_MainFolder + str(tmin) + '_' + str(tmax) + '\\\\'\n",
    "print('Data folder: ', dataFolder)\n",
    "\n",
    "source_MainFolder = \"..\\SourceLocalization\\SourceEstimates\\Data=\"\n",
    "sourceFolder = source_MainFolder + str(tmin) + '_' + str(tmax) + '\\\\'\n",
    "print('Source folder: ', sourceFolder)\n",
    "\n",
    "subjects_dir = '..\\SourceLocalization\\subjects\\\\'\n",
    "print('Subjects directory: ', subjects_dir)\n",
    "\n",
    "forwardModelsFolder = '..\\SourceLocalization\\ForwardModels\\\\'\n",
    "print('Forward models folder: ', forwardModelsFolder)\n",
    "\n",
    "spatialFiltersFolder = '..\\SourceLocalization\\SpatialFilters\\\\Data='\n",
    "spatialFiltersFolder = spatialFiltersFolder + str(tmin) + '_' + str(tmax) + '\\\\'\n",
    "print('Spatial filters folder: ', spatialFiltersFolder)\n",
    "\n",
    "statResultsFolder  = '..\\SourceLocalization\\Results\\\\Data='\n",
    "statResultsFolder = statResultsFolder + str(tmin) + '_' + str(tmax) + '\\\\'\n",
    "print('Statisctics results folder: ', statResultsFolder)\n",
    "\n",
    "classifiers_MainFolder = \"..\\Classifiers\\Data=\"    \n",
    "clsfFolder = classifiers_MainFolder + str(tmin) + '_' + str(tmax) + '\\\\'\n",
    "print('Classifiers folder: ', clsfFolder)\n",
    "    \n",
    "\n",
    "# Pick the classifiers based on the their training window\n",
    "if tmin == -0.8:\n",
    "    peak_indices = [90, 120]\n",
    "elif tmin == -0.1:\n",
    "    peak_indices = [20, 50]\n",
    "    \n",
    "print('Peak indices: ', peak_indices)\n",
    "    \n",
    "# Task name for the classifiers\n",
    "task_name = 'all_predLevel'\n",
    "print('Task name: ', task_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_ext = ''\n",
    "if tmin == -0.8 and tmax == 0.6:\n",
    "    filename_ext = '-elongated'\n",
    "elif tmin == -0.8 and tmax == 1:\n",
    "    filename_ext = '-elongated_2'\n",
    "    \n",
    "print('filename ext: ', filename_ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All subjects\n",
    "s_id_list_all = ['13', '16', '17', '18', '21', '23', '26', '28', '29', '30', \n",
    "                 '31', '32', '33', '34', '35', '36', '38', '39', '40', '41', '42']\n",
    "\n",
    "\n",
    "participant_names = ['Vinitski_D', 'Scherbakova_E', 'Ganbarov_E', 'Kasa_I' , 'Matuilko_I', 'Riaz_A', 'Solev_I',\n",
    "                     'Kolzhanov_N', 'Kabanova_V', 'Mochenova_M', 'Novikov_A', 'Adamovich_T', 'Gornostaeva_E',\n",
    "                     'Kazakov_I', 'Glebko', 'korobova', 'Ziberova_A', 'Prizhimova_E', 'Voronova_A', 'Saidov_F',\n",
    "                     'Bolgina_T']\n",
    "\n",
    "\n",
    "print('Number of subjects: ', len(participant_names))\n",
    "\n",
    "if os.path.exists(subjects_dir + '\\\\' + participant_names[0] + '\\surf\\\\'):  \n",
    "    print('exists!')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#participant_names[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEpochData(s_id):\n",
    "    \n",
    "   \n",
    "    if int(s_id) < 23:\n",
    "        fname = dataFolder+'S'+s_id+'\\\\'+s_id+'_2_tsss_mc_trans_'+sensors+'_nobase-epochs_afterICA'+filename_ext+'_manually_AR_resampled.fif'\n",
    "    else: \n",
    "        fname = dataFolder+'S'+s_id+'\\\\block_2_tsss_mc_trans_'+sensors+'_nobase-epochs_afterICA'+filename_ext+'_manually_AR_resampled.fif'\n",
    "\n",
    "\n",
    "    epochs = mne.read_epochs(fname, verbose='error')\n",
    "    print(fname + ' loaded!')\n",
    "    return epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getClassifierWeghts(filename):\n",
    "\n",
    "    print('Classifier is loaded from ', filename)\n",
    "    loaded_model = []\n",
    "    with open(filename, \"rb\") as f:\n",
    "        while True:\n",
    "            try:\n",
    "                loaded_model.append(pickle.load(f))\n",
    "            except EOFError:\n",
    "                break\n",
    "        \n",
    "    return loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitData(epochs, events=None):\n",
    "    #print(epochs.event_id)\n",
    "    if events == None:\n",
    "        print('No events are given as parameter!')\n",
    "    \n",
    "    else:\n",
    "        print('Requested events: ', events)\n",
    "        return_list = epochs[events]\n",
    "    \n",
    "    print('Events in return list: ', return_list.event_id)\n",
    "    \n",
    "    return return_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inverse modeling with BeamFormer on evoked data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beamformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute spatial filters for beamformer\n",
    "\n",
    "def computeSpatialFilter(s_name, s_id, evoked, noise_cov, data_cov, condName):\n",
    "    print('Beamformer')\n",
    "    print('Computing spatial filter..')\n",
    "    \n",
    "    # Load forward model\n",
    "    fname_fwd = forwardModelsFolder + '\\\\fwd_sol_ico5_' + s_name + '.fif'\n",
    "    fwd = mne.read_forward_solution(fname_fwd, verbose=False)\n",
    "    \n",
    "    # Compute spatial filters with evoked data, forward model and covariance matrices\n",
    "    filters = mne.beamformer.make_lcmv(evoked.info, fwd, data_cov=data_cov, reg=0.05, noise_cov=noise_cov, \n",
    "                                       pick_ori='max-power', weight_norm='unit-noise-gain', rank='full', reduce_rank=True)\n",
    "   \n",
    "    # Save filters\n",
    "    filters.save(spatialFiltersFolder + s_id + '_filters-lcmv_' + s_name + '.h5', overwrite=True)\n",
    "    return filters\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Morph subject's source estimate to template subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def morphToCommonSpace(stc, s_name, src_ave, smoothAmount=None):\n",
    "    print('Computing source morph..')\n",
    "    # Read the source space we are morphing to\n",
    "    fsave_vertices = [s['vertno'] for s in src_ave]\n",
    "\n",
    "    morph = mne.compute_source_morph(src=stc, subject_from=s_name, subject_to='fsaverage', \n",
    "                                     spacing=fsave_vertices, subjects_dir=subjects_dir, verbose=False,\n",
    "                                     smooth=smoothAmount)\n",
    "\n",
    "\n",
    "    tstep = stc.tstep\n",
    "    \n",
    "    print('Morphing data to fsaverage..')\n",
    "    stc_fsave = morph.apply(stc)\n",
    "    n_vertices_fsave = morph.morph_mat.shape[0]\n",
    "    \n",
    "    \n",
    "    return stc_fsave, n_vertices_fsave, tstep\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeEvoked_averagedTimeRange(epochs, timeRange):\n",
    "    \n",
    "    epochs = epochs.crop(tmin=timeRange[0], tmax=timeRange[1])\n",
    "    epochs_data = epochs.get_data()\n",
    "    print('epochs data shape: ', epochs_data.shape)\n",
    "    \n",
    "    epochs_data_avgTimeRange = np.mean(epochs_data, axis=2)\n",
    "    print('epochs_data_avgTimeRange shape: ', epochs_data_avgTimeRange.shape)\n",
    "    \n",
    "    evoked_data = np.mean(epochs_data_avgTimeRange, axis=0)\n",
    "    evoked_data = np.reshape(evoked_data, (len(evoked_data), 1))\n",
    "    print('evoked_data shape: ', evoked_data.shape)\n",
    "    \n",
    "    evoked = mne.EvokedArray(evoked_data, epochs.info)\n",
    "    \n",
    "    return evoked\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the inverse solution for group average\n",
    "def prepareInverseSolution_group(data, tstep, tmin_tmp=0):\n",
    "    \n",
    "    src_ave = mne.read_source_spaces(subjects_dir+'fsaverage\\\\bem\\\\fsaverage-ico-5-src.fif')\n",
    "\n",
    "    fsave_vertices = [s['vertno'] for s in src_ave]        \n",
    "            \n",
    "    stc_return = mne.SourceEstimate(data, fsave_vertices, tmin_tmp, tstep, subject='fsaverage')\n",
    "    \n",
    "    \n",
    "    return stc_return\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The STC (Source Time Courses) are defined on a source space formed by 7498 candidate locations and for a duration spanning 106 time instants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warning: Slide Type\n",
    "!!PQt5 is necessary and also run jupyter nbextension enable mayavi --py on command line before running the jupyter notebooks and also latest (6.1.1) version of module called 'traits'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlab.init_notebook('png')\n",
    "mne.viz.set_3d_backend('mayavi') #pyvista\n",
    "def showResult(s_id, sourceFolder, stc, condName, minimum, maximum, mid, tmin_tmp=0, \n",
    "               transparent=True, cmap='mne', alpha=1.0, center=1):\n",
    "    \n",
    "    initial_time = tmin_tmp\n",
    "    hemi_list = ['rh', 'lh']\n",
    "    for hemi in hemi_list:\n",
    "        print('Hemi: ', hemi)\n",
    "        \n",
    "        kwargs = dict(initial_time=initial_time, surface='inflated', hemi=hemi, subjects_dir=subjects_dir,\n",
    "                      verbose=False, size=(600, 600), spacing='all', background='w',\n",
    "                      cortex=(211/256,211/256,211/256), colorbar=True) #seismic coolwarm\n",
    "        \n",
    "        #plt.clim(minimum, maximum) \n",
    "        if cmap  == 'mne' or  cmap == 'Reds':\n",
    "             brain = stc.plot(**kwargs, colormap=cmap, clim=dict(kind='value', lims=[minimum, mid, maximum]))  \n",
    "        else:\n",
    "            brain = stc.plot(**kwargs, colormap=cmap, clim=dict(kind='value', pos_lims=[minimum, mid, maximum]))        \n",
    "        #brain.scale_data_colormap(fmin=minimum, fmid=mid, fmax=maximum, \n",
    "        #                             transparent=transparent, alpha=alpha, center=center)\n",
    "        \n",
    "   \n",
    "        brain.show_view('lateral');\n",
    "\n",
    "        brain.save_image(sourceFolder + s_id + '_' + hemi + '_' + condName + '.png')\n",
    "\n",
    "        Image(filename = sourceFolder +  s_id + '_' + hemi + '_' + condName + '.png', width=600)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Beamformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne.decoding import get_coef\n",
    "def computeActivationMaps(model_list, epochs, tmin):\n",
    "\n",
    "    meg_data = epochs.get_data()\n",
    "    epochs.average().plot()\n",
    "    print(\"Meg data shape: \", meg_data.shape)\n",
    "    \n",
    "    # get classifier weights\n",
    "    if len(model_list) > 0:\n",
    "        model = model_list[0] # if model is loaded, it is stored in a list. Therefore we need to get model from index 0\n",
    "        \n",
    "        # Get classifier weights\n",
    "        coef = get_coef(model, 'coef_', inverse_transform=True)\n",
    "        \n",
    "        # Compute mean and std of weights\n",
    "        coef_mean = np.mean(coef)\n",
    "        coef_std = np.std(coef)\n",
    "        \n",
    "        # Standardize the weights\n",
    "        coef = (coef-coef_mean)/coef_std\n",
    "        print('shape of coef: ', coef.shape)\n",
    "    \n",
    "    \n",
    "    # Multiplying classifier weights with covariance of data to compute activation maps\n",
    "    activations_mat = np.zeros((meg_data.shape[0], meg_data.shape[1], meg_data.shape[2]))\n",
    "    # ntrials, nchannels, ntimes\n",
    "    \n",
    "    for t in range(meg_data.shape[2]):\n",
    "        epochs_tmp = epochs.copy()\n",
    "        epochs_tmp.crop(tmin=epochs.times[t], tmax=epochs.times[t])\n",
    "        cov_tmp = mne.compute_covariance(epochs_tmp, verbose=False)\n",
    "\n",
    "        activations = np.dot(coef, cov_tmp.data)\n",
    "        if t == 0:\n",
    "            print('Shape of activations: ', activations.shape)\n",
    "        \n",
    "        for i in range( meg_data.shape[0]):\n",
    "            activations_mat[i, :,t] = activations.reshape(meg_data.shape[1])\n",
    "            \n",
    "        del cov_tmp \n",
    "    \n",
    "    # Simulate epoch object with activation maps\n",
    "    epoched_sim = mne.EpochsArray(activations_mat, epochs.info, tmin=tmin)\n",
    "\n",
    "    return epoched_sim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics to compare conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeStatistic(x, y):\n",
    " \n",
    "    print('comparing 2 groups')\n",
    "    '''\n",
    "    for i in range(x.shape[0]):\n",
    "        #print('vertex ind: ', i)\n",
    "        stats, pval = scipy.stats.ttest_rel(x[i,:], y[i,:])\n",
    "        \n",
    "        stats_array[i] = stats\n",
    "        pval_array[i] = pval\n",
    "    '''\n",
    "    stats_array, pval_array = stats.ttest_rel(x, y, axis=1)\n",
    "\n",
    "    return stats_array, pval_array\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def applyBeamformer(conditions, s_id_list_all, n_subjects, participant_names,\n",
    "                    tminData, tmaxData, tminNoise, tmaxNoise, tminEpoch, smoothAmount, task_name):\n",
    "    \n",
    "    stc_fsave_all, n_times = None,  None\n",
    "\n",
    "\n",
    "    for s in range(n_subjects): \n",
    "        s_id = s_id_list_all[s]\n",
    "        s_name = participant_names[s]\n",
    "        print(' ------------- ' + s_name + ' ------------- ')\n",
    "        epochs = getEpochData(s_id)\n",
    "        print(epochs.event_id)\n",
    "        print('epochs shape: ', epochs.get_data().shape)\n",
    "        \n",
    "        # check if all conditions exist in the epoch (e.g. omissions_living_nores might not exist!)\n",
    "        conditions = [c for c in conditions if c in epochs.event_id]\n",
    "        print('Final conditions: ', conditions)\n",
    "        splits = epochs[conditions]\n",
    "        print('Events in splits: ', splits.event_id)\n",
    "        \n",
    "                \n",
    "        # Load classifier weights to compute activation maps\n",
    "        clsf_model_filename = clsfFolder+'S'+s_id+'\\\\'+s_id+'_clsf_'+task_name+'_'+str(peak_indices[0])+'_'+str(peak_indices[1])+'_sm.sav'\n",
    "        clsf_model = getClassifierWeghts(clsf_model_filename)\n",
    "\n",
    "        # compute activation maps and simulate epoch object for source localization\n",
    "        print('Compute activations')\n",
    "        epoch_sim = computeActivationMaps(clsf_model, splits, tmin=tminEpoch)\n",
    "\n",
    "        print('Compute noise covariance')\n",
    "        noise_cov = mne.compute_covariance(epoch_sim, tmin=tminNoise, tmax=tmaxNoise, \n",
    "                                           method=['shrunk', 'empirical'], verbose=False) \n",
    "        print('Compute data covariance')\n",
    "        data_cov = mne.compute_covariance(epoch_sim, tmin=tminData, tmax=tmaxData, \n",
    "                                          method=['shrunk', 'empirical'], verbose=False)\n",
    "\n",
    "        # compute average of epochs\n",
    "        evoked = epoch_sim.average().crop(tmin=tminData, tmax=tmaxData)\n",
    "        print('Shape of evoked data: ', evoked._data.shape)\n",
    "\n",
    "        # computer spatial filters by LCMV\n",
    "        print('Compute filter: ')\n",
    "        filters = computeSpatialFilter(s_name, s_id, evoked, noise_cov, data_cov, conditions)\n",
    "        \n",
    "        print('Apply beamformer: ')\n",
    "        stc = mne.beamformer.apply_lcmv(evoked=evoked, filters=filters, max_ori_out='signed')\n",
    "        n_vertices_sample, n_times = stc.data.shape\n",
    "       \n",
    "\n",
    "        if s_id != '16' and s_id != '31' and s_id != '40':\n",
    "            stc_fsave, n_vertices_fsave, tstep = morphToCommonSpace(stc, s_name, src_ave,\n",
    "                                                                    smoothAmount=smoothAmount)\n",
    "        else:\n",
    "            stc_fsave = stc\n",
    "            \n",
    "        \n",
    "        if s==0:\n",
    "            print('n_times: ', n_times)\n",
    "            stc_fsave_all = np.zeros((n_vertices_fsave, n_times, n_subjects),)\n",
    "\n",
    "        \n",
    "        stc_fsave_all[:,:,s] = np.abs(stc_fsave.data.reshape(n_vertices_fsave, n_times))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return  stc_fsave_all, n_times, tstep\n",
    "                    \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read fsaverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read fsaverage\n",
    "src_ave = mne.read_source_spaces(subjects_dir + 'fsaverage\\\\bem\\\\fsaverage-ico-5-src.fif')\n",
    "#fsave_vertices = [s['vertno'] for s in src_ave]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Choose the conditions for contrasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions_8 = ['living_omission_8_nores', 'living_omission_8_corr', 'living_omission_8_incorr',\n",
    "                 'object_omission_8_nores', 'object_omission_8_corr', 'object_omission_8_incorr']\n",
    "\n",
    "conditions_10 = ['living_omission_10_nores', 'living_omission_10_corr', 'living_omission_10_incorr',\n",
    "                 'object_omission_10_nores', 'object_omission_10_corr', 'object_omission_10_incorr']\n",
    "condition_names =  [\"omi\"]\n",
    "\n",
    "n_subjects = len(s_id_list_all)\n",
    "print('number of subjects: ', n_subjects)\n",
    "\n",
    "print('conditions 80%: \\n', conditions_8)\n",
    "print('conditions 100%: \\n', conditions_10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define time limits for inverse solutins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tminData, tmaxData = 0.1, 0.4  #0.1, 0.4 #-0.04, 0\n",
    "print('tmin for data: ', tminData)\n",
    "print('tmax for data: ', tmaxData)\n",
    "tminNoiseCov, tmaxNoiseCov = -0.1, -0.05\n",
    "print('tmin for noise covariance: ', tminNoiseCov)\n",
    "print('tmax for noise covariance: ', tmaxNoiseCov)\n",
    "tminEpoch = -0.1\n",
    "print('tmin for generated epochs: ', tminEpoch)\n",
    "smoothAmount = 70\n",
    "print('Smoothing amount: ', smoothAmount)\n",
    "inv_sol_method = 'beamformer'\n",
    "print('Inverse solution method: ', inv_sol_method)\n",
    "tstep=0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use beamformer for computing source estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('======= Applying Beamformer ========')\n",
    "\n",
    "stc_fsave_omi_8, n_times, tstep = applyBeamformer(conditions_8, s_id_list_all, \n",
    "                                                     n_subjects, participant_names, tminData, tmaxData, \n",
    "                                                     tminNoiseCov, tmaxNoiseCov, tminEpoch, smoothAmount, task_name)\n",
    "\n",
    "\n",
    "stc_omi_8_filename = 'stc_fsave_omi_8_onActivationMaps_'+inv_sol_method+'_'+str(tminData)+'_'+str(tmaxData)+'_sm='+str(smoothAmount)+'.npy'\n",
    "\n",
    "print('stc omi is saved in ', stc_omi_8_filename)\n",
    "np.save(statResultsFolder + stc_omi_8_filename, stc_fsave_omi_8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "100%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('======= Applying Beamformer ========')\n",
    "\n",
    "stc_fsave_omi_10, n_times, tstep = applyBeamformer(conditions_10, s_id_list_all, \n",
    "                                                     n_subjects, participant_names, tminData, tmaxData, \n",
    "                                                     tminNoiseCov, tmaxNoiseCov, tminEpoch, smoothAmount, task_name)\n",
    "\n",
    "\n",
    "stc_omi_10_filename = 'stc_fsave_omi_10_onActivationMaps_'+inv_sol_method+'_'+str(tminData)+'_'+str(tmaxData)+'_sm='+str(smoothAmount)+'.npy'\n",
    "\n",
    "print('stc omi is saved in ', stc_omi_10_filename)\n",
    "np.save(statResultsFolder + stc_omi_10_filename, stc_fsave_omi_10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inv_sol_method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you have the source estimates values saved already, load them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cls time range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tminData_cls_tmp, tmaxData_cls_tmp = 0.1, 0.4\n",
    "\n",
    "# 80%\n",
    "stc_omi_8_filename = 'stc_fsave_omi_8_onActivationMaps_'+inv_sol_method+'_'+str(tminData_cls_tmp)+'_'+str(tmaxData_cls_tmp)+'_sm='+str(smoothAmount)+'.npy'\n",
    "print(statResultsFolder+stc_omi_8_filename)\n",
    "stc_fsave_omi_8_clfRange = np.load(statResultsFolder+stc_omi_8_filename)\n",
    "\n",
    "\n",
    "print('shape of omi 80% source estimates: ', stc_fsave_omi_8_clfRange.shape)\n",
    "print(np.where(stc_fsave_omi_8_clfRange == 0))\n",
    "\n",
    "\n",
    "# 100%\n",
    "stc_omi_10_filename = 'stc_fsave_omi_10_onActivationMaps_'+inv_sol_method+'_'+str(tminData_cls_tmp)+'_'+str(tmaxData_cls_tmp)+'_sm='+str(smoothAmount)+'.npy'\n",
    "print(statResultsFolder+stc_omi_10_filename)\n",
    "stc_fsave_omi_10_clfRange = np.load(statResultsFolder+stc_omi_10_filename)\n",
    "\n",
    "\n",
    "print('shape of omi 100% source estimates: ', stc_fsave_omi_10_clfRange.shape)\n",
    "print(np.where(stc_fsave_omi_10_clfRange == 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tminData_baseline_tmp, tmaxData_baseline_tmp = -0.04, 0\n",
    "\n",
    "# 80%\n",
    "stc_omi_8_filename_baseline = 'stc_fsave_omi_8_onActivationMaps_'+inv_sol_method+'_'+str(tminData_baseline_tmp)+'_'+str(tmaxData_baseline_tmp)+'_sm='+str(smoothAmount)+'.npy'\n",
    "print(stc_omi_8_filename_baseline)\n",
    "stc_fsave_omi_8_baseline = np.load(statResultsFolder+stc_omi_8_filename_baseline)\n",
    "print('shape of omi 80% baseline source estimates: ', stc_fsave_omi_8_baseline.shape)\n",
    "print(np.where(stc_fsave_omi_8_baseline == 0))\n",
    "\n",
    "# 100%\n",
    "stc_omi_10_filename_baseline = 'stc_fsave_omi_10_onActivationMaps_'+inv_sol_method+'_'+str(tminData_baseline_tmp)+'_'+str(tmaxData_baseline_tmp)+'_sm='+str(smoothAmount)+'.npy'\n",
    "print(stc_omi_10_filename_baseline)\n",
    "stc_fsave_omi_10_baseline = np.load(statResultsFolder+stc_omi_10_filename_baseline)\n",
    "print('shape of omi 100% baseline source estimates: ', stc_fsave_omi_10_baseline.shape)\n",
    "print(np.where(stc_fsave_omi_10_baseline == 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check data before doing anything!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#stc_fsave_omi_8_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#stc_fsave_omi_10_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#stc_fsave_omi_8_clfRange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#stc_fsave_omi_10_clfRange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stc_fsave_omi_8_clfRange.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute group-level relative change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Take mean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80%\n",
    "# take mean over participants (dimension 2)\n",
    "stc_fsave_omi_8_clfRange_avg = np.mean(stc_fsave_omi_8_clfRange, axis=2)\n",
    "print('Shape stc_fsave_omi_8_clfRange_avg after avg over subjects: ', stc_fsave_omi_8_clfRange_avg.shape)\n",
    "# take mean over time points (dimension 1)\n",
    "stc_fsave_omi_8_clfRange_avg= np.mean(stc_fsave_omi_8_clfRange_avg, axis=1)\n",
    "print('Shape stc_fsave_omi_8_clfRange_avg after avg across time: ', stc_fsave_omi_8_clfRange_avg.shape)\n",
    "print('stc_fsave_omi_8_clfRange_avg shape: ', stc_fsave_omi_8_clfRange_avg.shape)\n",
    "\n",
    "print('stc_fsave_omi_8_clfRange_avg: ', stc_fsave_omi_8_clfRange_avg)\n",
    "\n",
    "# take mean over participants (dimension 2)\n",
    "stc_fsave_omi_8_baseline_avg = np.mean(stc_fsave_omi_8_baseline, axis=2)\n",
    "print('Shape stc_fsave_omi_8_baseline_avg after avg over subjects: ', stc_fsave_omi_8_baseline_avg.shape)\n",
    "# take mean over time points (dimension 1)\n",
    "stc_fsave_omi_8_baseline_avg = np.mean(stc_fsave_omi_8_baseline_avg, axis=1)\n",
    "print('Shape stc_fsave_omi_8_baseline_avg after avg across time: ', stc_fsave_omi_8_baseline_avg.shape)\n",
    "print('stc_fsave_omi_8_baseline_avg shape: ', stc_fsave_omi_8_baseline_avg.shape)\n",
    "\n",
    "print('stc_fsave_omi_8_baseline_avg: ', stc_fsave_omi_8_baseline_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100%\n",
    "# take mean over participants (dimension 2)\n",
    "stc_fsave_omi_10_clfRange_avg = np.mean(stc_fsave_omi_10_clfRange, axis=2)\n",
    "print('Shape stc_fsave_omi_10_clfRange_avg after avg over subjects: ', stc_fsave_omi_10_clfRange_avg.shape)\n",
    "# take mean over time points (dimension 1)\n",
    "stc_fsave_omi_10_clfRange_avg= np.mean(stc_fsave_omi_10_clfRange_avg, axis=1)\n",
    "print('Shape stc_fsave_omi_10_clfRange_avg after avg across time: ', stc_fsave_omi_10_clfRange_avg.shape)\n",
    "print('stc_fsave_omi_10_clfRange_avg shape: ', stc_fsave_omi_10_clfRange_avg.shape)\n",
    "\n",
    "print('stc_fsave_omi_10_clfRange_avg: ', stc_fsave_omi_10_clfRange_avg)\n",
    "\n",
    "# take mean over participants (dimension 2)\n",
    "stc_fsave_omi_10_baseline_avg = np.mean(stc_fsave_omi_10_baseline, axis=2)\n",
    "print('Shape stc_fsave_omi_10_baseline_avg after avg over subjects: ', stc_fsave_omi_10_baseline_avg.shape)\n",
    "# take mean over time points (dimension 1)\n",
    "stc_fsave_omi_10_baseline_avg = np.mean(stc_fsave_omi_10_baseline_avg, axis=1)\n",
    "print('Shape stc_fsave_omi_10_baseline_avg after avg across time: ', stc_fsave_omi_10_baseline_avg.shape)\n",
    "print('stc_fsave_omi_10_baseline_avg shape: ', stc_fsave_omi_10_baseline_avg.shape)\n",
    "\n",
    "print('stc_fsave_omi_10_baseline_avg: ', stc_fsave_omi_10_baseline_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Generate stc group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#80%\n",
    "stc_cls_8 = prepareInverseSolution_group(stc_fsave_omi_8_clfRange_avg, tstep=tstep, tmin_tmp=tminData_cls_tmp)\n",
    "stc_baseline_8 = prepareInverseSolution_group(stc_fsave_omi_8_baseline_avg, tstep=tstep,\n",
    "                                            tmin_tmp=tminData_baseline_tmp)\n",
    "\n",
    "#100%\n",
    "stc_cls_10 = prepareInverseSolution_group(stc_fsave_omi_10_clfRange_avg, tstep=tstep, tmin_tmp=tminData_cls_tmp)\n",
    "stc_baseline_10 = prepareInverseSolution_group(stc_fsave_omi_10_baseline_avg, tstep=tstep,\n",
    "                                            tmin_tmp=tminData_baseline_tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showResult('fsaverage', sourceFolder, stc_cls_8,\n",
    "           'omi_8_clf_'+inv_sol_method+'_sm='+str(smoothAmount), \n",
    "           minimum=1300000, maximum=3000000)\n",
    "\n",
    "showResult('fsaverage', sourceFolder, stc_baseline_8, \n",
    "           'omi_8_baseline_'+inv_sol_method+'_sm='+str(smoothAmount), \n",
    "           minimum=99000, maximum=440000)\n",
    "\n",
    "showResult('fsaverage', sourceFolder, stc_cls_10,\n",
    "           'omi_10_clf_'+inv_sol_method+'_sm='+str(smoothAmount), \n",
    "           minimum=1300000, maximum=3000000)\n",
    "\n",
    "showResult('fsaverage', sourceFolder, stc_baseline_10, \n",
    "           'omi_10_baseline_'+inv_sol_method+'_sm='+str(smoothAmount), \n",
    "           minimum=99000, maximum=440000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Average relative change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#80% avg\n",
    "\n",
    "stc_fsave_omi_8_diff_avg = 100*(stc_fsave_omi_8_clfRange_avg-stc_fsave_omi_8_baseline_avg)/stc_fsave_omi_8_baseline_avg\n",
    "\n",
    "\n",
    "# 100% avg\n",
    "\n",
    "stc_fsave_omi_10_diff_avg = 100*(stc_fsave_omi_10_clfRange_avg-stc_fsave_omi_10_baseline_avg)/stc_fsave_omi_10_baseline_avg\n",
    "\n",
    "print('Minimum of 80%: ', min(stc_fsave_omi_8_diff_avg))\n",
    "print('Maximum of 80%: ', max(stc_fsave_omi_8_diff_avg))\n",
    "\n",
    "print('Minimum of 100%: ', min(stc_fsave_omi_10_diff_avg))\n",
    "print('Maximum of 100%: ', max(stc_fsave_omi_10_diff_avg))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#thresh=145\n",
    "thresh_8 = 770 #160\n",
    "thresh_10 = 770\n",
    "print(np.where(stc_fsave_omi_8_diff_avg >thresh_8)[0].shape)\n",
    "print(np.where(stc_fsave_omi_10_diff_avg >thresh_10)[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80%\n",
    "for i in range(stc_fsave_omi_8_diff_avg.shape[0]):\n",
    "    if stc_fsave_omi_8_diff_avg[i] < thresh_8:\n",
    "        stc_fsave_omi_8_diff_avg[i] = 0\n",
    "        \n",
    "# 100%\n",
    "for i in range(stc_fsave_omi_10_diff_avg.shape[0]):\n",
    "    if stc_fsave_omi_10_diff_avg[i] < thresh_10:\n",
    "        stc_fsave_omi_10_diff_avg[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stc_diff_8 = prepareInverseSolution_group(stc_fsave_omi_8_diff_avg, tstep=tstep, tmin_tmp=tminData_cls_tmp)\n",
    "stc_diff_10 = prepareInverseSolution_group(stc_fsave_omi_10_diff_avg, tstep=tstep, tmin_tmp=tminData_cls_tmp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " '2omi_8_change_'+inv_sol_method+'_sm='+str(smoothAmount) +'_pthresh=' + str(thresh_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#80%\n",
    "print('Minimum of 80%: ', min(stc_diff_8.data))\n",
    "print('Maximum of 80%: ', max(stc_diff_8.data))\n",
    "showResult('fsaverage', sourceFolder, stc_diff_8, \n",
    "           '2omi_8_change_'+inv_sol_method+'_sm='+str(smoothAmount) +'_pthresh=' + str(thresh_8), \n",
    "           minimum=600, maximum=1100, transparent=True, cmap='Oranges')\n",
    "\n",
    "#100%\n",
    "print('Minimum of 100%: ', min(stc_diff_10.data))\n",
    "print('Maximum of 100%: ', max(stc_diff_10.data))\n",
    "showResult('fsaverage', sourceFolder, stc_diff_10, \n",
    "           '2omi_10_change_'+inv_sol_method+'_sm='+str(smoothAmount) +'_pthresh=' + str(thresh_10), \n",
    "           minimum=600, maximum=1100, transparent=True, cmap='Oranges')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find the mni coordinates of the significant blob in the plot comparing 80  and 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Get labels of parcellation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sd = '/Users/athina/Documents/Projects/Pinar_MEG/'\n",
    "\n",
    "src_ave = mne.read_source_spaces(subjects_dir+'fsaverage\\\\bem\\\\fsaverage-ico-5-src.fif')\n",
    "\n",
    "fsave_vertices = [s['vertno'] for s in src_ave]\n",
    "\n",
    "label = mne.read_labels_from_annot('fsaverage', parc='aparc',hemi = 'both',\n",
    "                                   subjects_dir=subjects_dir)\n",
    "print('All labels: ')\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list of labels:\n",
    "\n",
    "labels_names = stc_f_omi.data.shape[0]*[None]\n",
    "labels_inSourceEst = stc_f_omi.data.shape[0]*[None]\n",
    "\n",
    "for l_i in range(len(label)):\n",
    "\n",
    "#    l_i = 0 # label index we want\n",
    "\n",
    "    # the name for current labels:\n",
    "    for ver in label[l_i].restrict(src_ave).vertices:\n",
    "        labels_names[ver] = label[l_i].restrict(src_ave).name\n",
    "        labels_inSourceEst[ver] = label[l_i].restrict(src_ave)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_thres = 0 # find the peak for t-values supporting 100% > 80%\n",
    "\n",
    "sign_i = np.where(stc_f_omi.data > desired_thres)[0]\n",
    "labels_names_sign = []\n",
    "for s in sign_i:\n",
    "    print(labels_names[s])\n",
    "    labels_names_sign.append(labels_names[s])\n",
    "    \n",
    "print('Number of significant labels', len(sign_i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hemi = 'rh'\n",
    "hemi_ind = 1 #rh\n",
    "\n",
    "    \n",
    "# find the peak on the rh\n",
    "peak_vertex, peak_time = stc_f_omi.get_peak(hemi=hemi)\n",
    "print('peak_vertex: ', peak_vertex)\n",
    "\n",
    "# get the vertex at the peak\n",
    "peak_vertex_surf = stc_f_omi.rh_vertno[peak_vertex]\n",
    "\n",
    "# plot the source estimate with the peak painted blue\n",
    "kwargs = dict(initial_time=0.1, surface='inflated', hemi=hemi, subjects_dir=subjects_dir,\n",
    "                      verbose=True, size=(600, 600), spacing='all', background='w', \n",
    "              cortex=(211/256,211/256,211/256)) #, clim=dict(kind='value', pos_lims=(10, 100, 400))\n",
    "        \n",
    "brain = stc_f_omi.plot(**kwargs, colormap='Oranges')\n",
    "brain.scale_data_colormap(fmin=v_mi, fmid=(v_mi+v_ma)/2, fmax=v_ma, transparent=True)\n",
    "\n",
    "# add the vertex at the peak to the plot\n",
    "#brain.add_foci(peak_vertex_surf, coords_as_verts=True, hemi=hemi, color='blue')\n",
    "\n",
    "view = 'lateral' #medial lateral\n",
    "brain.show_view(view);\n",
    "brain.save_image(sourceFolder +  'onActivationMaps_omi_8vs10_' + hemi + '_with_maxpoint_'+view+'.png')\n",
    "\n",
    "# convert vertex to MNI coordinates\n",
    "coordinate = mne.vertex_to_mni(peak_vertex_surf, hemis=hemi_ind, subject='fsaverage', subjects_dir=subjects_dir)\n",
    "\n",
    "print('coordinates: ', coordinate)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute relative change for each participant at 80 and 100 level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Compute relative change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#80%\n",
    "# collapse time range by averaging over time points -> output will have the size of n_vertices x nsubjects\n",
    "stc_fsave_omi_8_clfRange_avgOverTime = np.mean(stc_fsave_omi_8_clfRange, axis=1)\n",
    "stc_fsave_omi_8_baseline_avgOverTime = np.mean(stc_fsave_omi_8_baseline, axis=1)\n",
    "\n",
    "stc_fsave_omi_8_diff = np.zeros(stc_fsave_omi_8_clfRange_avgOverTime.shape)\n",
    "print('shape of stc_fsave_omi_8_diff: ', stc_fsave_omi_8_diff.shape)\n",
    "for j in range(stc_fsave_omi_8_diff.shape[1]):\n",
    "    stc_fsave_omi_8_diff[:,j] = 100*(stc_fsave_omi_8_clfRange_avgOverTime[:,j]-stc_fsave_omi_8_baseline_avgOverTime[:,j])/stc_fsave_omi_8_baseline_avgOverTime[:,j]\n",
    "\n",
    "print('stc_fsave_omi_8_diff: ', stc_fsave_omi_8_diff.shape)\n",
    "\n",
    "# 100%\n",
    "# collapse time range by averaging over time points -> output will have the size of n_vertices x nsubjects\n",
    "stc_fsave_omi_10_clfRange_avgOverTime = np.mean(stc_fsave_omi_10_clfRange, axis=1)\n",
    "stc_fsave_omi_10_baseline_avgOverTime = np.mean(stc_fsave_omi_10_baseline, axis=1)\n",
    "\n",
    "stc_fsave_omi_10_diff = np.zeros(stc_fsave_omi_10_clfRange_avgOverTime.shape)\n",
    "\n",
    "for j in range(stc_fsave_omi_10_diff.shape[1]):\n",
    "    stc_fsave_omi_10_diff[:,j] = 100*(stc_fsave_omi_10_clfRange_avgOverTime[:,j]-stc_fsave_omi_10_baseline_avgOverTime[:,j])/stc_fsave_omi_10_baseline_avgOverTime[:,j]\n",
    "\n",
    "print('stc_fsave_omi_10_diff: ', stc_fsave_omi_10_diff)  \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stc_fsave_omi_8_diff[0,0])\n",
    "print(stc_fsave_omi_10_diff[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(stc_fsave_omi_10_diff.shape[1]):\n",
    "    print('i: ', i)\n",
    "    print('N=', len(np.where(stc_fsave_omi_8_diff[:,i]-stc_fsave_omi_10_diff[:,i]>0)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Stats to contrast 80% predictability and 100% predictability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(fvalues_o_new[fvalues_o_new != 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pthresh = 0.05\n",
    "print('computing the stats..')\n",
    "\n",
    "#omissions\n",
    "fvalues_o, pvalues_o = computeStatistic(stc_fsave_omi_10_diff, stc_fsave_omi_8_diff)\n",
    "\n",
    "# save values for omissions\n",
    "fvalue_filename_o = statResultsFolder+'fvalues_all_omi_activations_8vs10_'+inv_sol_method+'_sm='+str(smoothAmount)+'.npy'\n",
    "print(fvalue_filename_o)\n",
    "pvalue_filename_o = statResultsFolder+'pvalues_all_omi_activations_8vs10_'+inv_sol_method+'_sm='+str(smoothAmount)+'.npy'\n",
    "print('saving stats...')\n",
    "np.save(fvalue_filename_o, fvalues_o)\n",
    "np.save(pvalue_filename_o, pvalues_o)\n",
    "\n",
    "print('Significant p vals omi: ', np.where(pvalues_o <= pthresh))\n",
    "fvalues_o_new = np.copy(fvalues_o)\n",
    "pvalues_o_new = np.copy(pvalues_o)\n",
    "\n",
    "\n",
    "\n",
    "for j in range(len(fvalues_o)):\n",
    "    #omissions\n",
    "    if pvalues_o[j] > pthresh:\n",
    "        pvalues_o_new[j] = 1\n",
    "        fvalues_o_new[j] = 0\n",
    "\n",
    "#omissions\n",
    "rejets_o, pvalues_o_corrected = mne.stats.fdr_correction(pvalues_o)\n",
    "print('omissions-corrrected:')\n",
    "print(np.where(pvalues_o_corrected<= pthresh))\n",
    "\n",
    "\n",
    "print('generate stc for stats..')\n",
    "# omissions\n",
    "stc_f_omi = prepareInverseSolution_group(fvalues_o_new, tstep=tstep)\n",
    "#stc_f_omi = prepareInverseSolution_group(1-pvalues_o_new, tstep=tstep)\n",
    "\n",
    "#omissions\n",
    "#lateral\n",
    "v_mi, v_ma = 0, 2.09  #-1.5, 1.5\n",
    "showResult('fsaverage', sourceFolder, stc_f_omi, \n",
    "           'pval_predLevel_omi_activations_8vs10_pthresholded_'+inv_sol_method+'_sm='+str(smoothAmount)+'_pthresh='+str(pthresh),\n",
    "           minimum=v_mi, maximum=v_ma, mid=(v_mi+v_ma)/2, transparent=False, cmap='coolwarm') #twilight_shifted\n",
    "\n",
    "\n",
    "#del fvalues_o, pvalues_o, pvalues_o_corrected, fvalues_o_new, stc_f_omi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('fvalues_o_new min: ', min(abs(fvalues_o_new[fvalues_o_new != 0])))\n",
    "print('fvalues_o_new max: ', max(abs(fvalues_o_new[fvalues_o_new != 0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= (1-pvalues_o_new)\n",
    "a[np.where(a != 0)[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get  mni coordinates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hemi = 'rh'\n",
    "hemi_ind = 1 #rh\n",
    "\n",
    "    \n",
    "# find the peak on the rh\n",
    "peak_vertex, peak_time = stc_f_omi.get_peak(hemi=hemi)\n",
    "print('peak_vertex: ', peak_vertex)\n",
    "\n",
    "# get the vertex at the peak\n",
    "peak_vertex_surf = stc_f_omi.rh_vertno[peak_vertex]\n",
    "\n",
    "\n",
    "# convert vertex to MNI coordinates\n",
    "coordinate = mne.vertex_to_mni(peak_vertex_surf, hemis=hemi_ind, subject='fsaverage', subjects_dir=subjects_dir)\n",
    "\n",
    "print('coordinates: ', coordinate)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mni coordinates of frontal part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sd = '/Users/athina/Documents/Projects/Pinar_MEG/'\n",
    "\n",
    "src_ave = mne.read_source_spaces(subjects_dir+'fsaverage\\\\bem\\\\fsaverage-ico-5-src.fif')\n",
    "\n",
    "fsave_vertices = [s['vertno'] for s in src_ave]\n",
    "\n",
    "label = mne.read_labels_from_annot('fsaverage', parc='aparc',hemi = 'both',\n",
    "                                   subjects_dir=subjects_dir)\n",
    "print('All labels: ')\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hemi = 'rh'\n",
    "hemi_ind = 1\n",
    "\n",
    "# find the labels of desired parcellations (e.g. frontal ones) \n",
    "labels_rh_frontal = [l for l in label if 'frontal' in l.name and hemi in l.name][2]\n",
    "print(labels_rh_frontal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restrict the area to find the peak in the preferred area\n",
    "peak_frontal_rh, peak_time = stc_f_omi.in_label(labels_rh_frontal).get_peak()\n",
    "print(peak_frontal_rh)\n",
    "\n",
    "# get vertex at the peak\n",
    "peak_vertex_surf_frontal_rh = stc_f_omi.rh_vertno[peak_frontal_rh]\n",
    "\n",
    "# get MNI coordinates of the vertex at the peak\n",
    "coordinate_frontal = mne.vertex_to_mni(stc_f_omi.rh_vertno[peak_vertex_surf_frontal_rh],\n",
    "                                        hemis=hemi_ind, subject='fsaverage',\n",
    "                                        subjects_dir=subjects_dir)\n",
    "\n",
    "print('coordinates frontal: ', coordinate_frontal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot source estimate with the peak painted blue\n",
    "kwargs = dict(initial_time=0.1, surface='inflated', hemi=hemi, subjects_dir=subjects_dir,\n",
    "                      verbose=True, size=(600, 600), spacing='all', background='w', \n",
    "                      cortex=(211/256,211/256,211/256))\n",
    "        \n",
    "brain = stc_f_omi.plot(**kwargs, colormap='Oranges')\n",
    "brain.add_foci(peak_vertex_surf_frontal_rh, coords_as_verts=True, hemi=hemi, color='blue')\n",
    "brain.scale_data_colormap(fmin=0, fmid=0.5, fmax=1, transparent=True)\n",
    "\n",
    "view = 'lateral' #medial\n",
    "brain.show_view(view);\n",
    "brain.save_image(sourceFolder +  '100vs80_' + hemi + '_with_maxpoint_'+view+'_frontal.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    \n",
    "# find the peak on the rh\n",
    "peak_vertex, peak_time = stc_f_omi.get_peak(hemi=hemi)\n",
    "print('peak_vertex: ', peak_vertex)\n",
    "\n",
    "# get the vertex at the peak\n",
    "peak_vertex_surf = stc_f_omi.rh_vertno[peak_vertex]\n",
    "\n",
    "# plot the source estimate with the peak painted blue\n",
    "kwargs = dict(initial_time=0.1, surface='inflated', hemi=hemi, subjects_dir=subjects_dir,\n",
    "                      verbose=True, size=(600, 600), spacing='all', background='w', \n",
    "              cortex=(211/256,211/256,211/256)) #, clim=dict(kind='value', pos_lims=(10, 100, 400))\n",
    "        \n",
    "brain = stc_f_omi.plot(**kwargs, colormap='Oranges')\n",
    "brain.scale_data_colormap(fmin=v_mi, fmid=(v_mi+v_ma)/2, fmax=v_ma, transparent=True)\n",
    "\n",
    "# add the vertex at the peak to the plot\n",
    "#brain.add_foci(peak_vertex_surf, coords_as_verts=True, hemi=hemi, color='blue')\n",
    "\n",
    "view = 'lateral' #medial lateral\n",
    "brain.show_view(view);\n",
    "brain.save_image(sourceFolder +  'onActivationMaps_omi_8vs10_' + hemi + '_with_maxpoint_'+view+'.png')\n",
    "\n",
    "# convert vertex to MNI coordinates\n",
    "coordinate = mne.vertex_to_mni(peak_vertex_surf, hemis=hemi_ind, subject='fsaverage', subjects_dir=subjects_dir)\n",
    "\n",
    "print('coordinates: ', coordinate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pvalues_o[np.where(pvalues_o <= 0.05)]\n",
    "#max(fvalues_o_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stc_fsave_omi_10_diff.shape)\n",
    "print(stc_fsave_omi_10_diff[:,0])\n",
    "print(stc_fsave_omi_8_diff[:,0])\n",
    "print((stc_fsave_omi_10_diff-stc_fsave_omi_8_diff).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Compute the difference between 80% and 100%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import cmasher as cmr\n",
    "cmap = plt.get_cmap('cmr.seasons')\n",
    "mask = 300\n",
    "\n",
    "stc_fsave_omi_diffBetween_10_8 = np.zeros((stc_fsave_omi_10_diff.shape[0], stc_fsave_omi_10_diff.shape[1]),)\n",
    "for i in range(stc_fsave_omi_10_diff.shape[0]):\n",
    "    for j in range(stc_fsave_omi_10_diff.shape[1]):\n",
    "        stc_fsave_omi_diffBetween_10_8[i,j] = stc_fsave_omi_10_diff[i,j]-stc_fsave_omi_8_diff[i,j]\n",
    "\n",
    "stc_fsave_omi_diffBetween_10_8_avg = np.mean(stc_fsave_omi_diffBetween_10_8, axis=1)\n",
    "stc_fsave_omi_diffBetween_10_8_avg_masked = stc_fsave_omi_diffBetween_10_8_avg.copy()\n",
    "\n",
    "for i in range(stc_fsave_omi_diffBetween_10_8_avg_masked.shape[0]):\n",
    "    if stc_fsave_omi_diffBetween_10_8_avg_masked[i] > 0:\n",
    "        stc_fsave_omi_diffBetween_10_8_avg_masked[i] = 1\n",
    "    else:\n",
    "        stc_fsave_omi_diffBetween_10_8_avg_masked[i] = -1\n",
    "\n",
    "\n",
    "print('Min of stc_fsave_omi_diffBetween_10_8_avg: ', min(stc_fsave_omi_diffBetween_10_8_avg))\n",
    "print('Max of stc_fsave_omi_diffBetween_10_8_avg: ', max(stc_fsave_omi_diffBetween_10_8_avg))\n",
    "\n",
    "stc_fsave_omi_diffBetween_10_8_avg_new = stc_fsave_omi_diffBetween_10_8_avg.copy()\n",
    "for s in range(stc_fsave_omi_diffBetween_10_8_avg_new.shape[0]):\n",
    "    if stc_fsave_omi_diffBetween_10_8_avg[s]  < mask and stc_fsave_omi_diffBetween_10_8_avg[s] > -mask:\n",
    "        stc_fsave_omi_diffBetween_10_8_avg_new[s] = 0\n",
    "        \n",
    "\n",
    "print(len(np.where(stc_fsave_omi_diffBetween_10_8_avg_new < 1)[0]))\n",
    "stc_fsave_omi_diffBetween_10_8_avg_source = prepareInverseSolution_group(stc_fsave_omi_diffBetween_10_8_avg_new,\n",
    "                                                                         tstep=tstep, tmin_tmp=tminData_cls_tmp)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "showResult('fsaverage', sourceFolder, stc_fsave_omi_diffBetween_10_8_avg_source, \n",
    "           'omi_diffBetween_10_8_'+inv_sol_method+'_sm='+str(smoothAmount) + 'mask=' + str(mask), \n",
    "           minimum=0, mid=500, maximum=1500, transparent=False, cmap='seismic') #, center=0) \n",
    "    #seismic twilight_shifted -500, maximum=1000 -  minimum=0, mid=500, maximum=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stc_fsave_omi_diffBetween_10_8_avg_new[stc_fsave_omi_diffBetween_10_8_avg_new >= 0].shape)\n",
    "\n",
    "print(stc_fsave_omi_diffBetween_10_8_avg_new[stc_fsave_omi_diffBetween_10_8_avg_new < 0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stc_fsave_omi_10_diff.shape\n",
    "np.where(stc_fsave_omi_10_diff[:,13] < 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stc_fsave_omi_10_diff.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stc_fsave_omi_diffBetween_10_8_avg[stc_fsave_omi_diffBetween_10_8_avg<-800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.mean(stc_fsave_omi_8_diff, axis=1)\n",
    "a[a > 800].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
